{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060daa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_y_pred.csv\n",
      "./train.csv\n",
      "./test_x.csv\n",
      "./text_Wu.ipynb\n",
      "./.ipynb_checkpoints/text_Wu-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25892d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2115</td>\n",
       "      <td>catastrophe</td>\n",
       "      <td>Florida</td>\n",
       "      <td>@deb117 7/30 that catastrophe man opens school...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9232</td>\n",
       "      <td>suicide%20bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meek mill should join isis since he loves suic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8131</td>\n",
       "      <td>rescued</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Three beached whales rescued in Kerry - http:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8236</td>\n",
       "      <td>riot</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Southeast Dirt Riot Series Crowns Champions:  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9822</td>\n",
       "      <td>trauma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hiroshima: They told me to paint my story: Eig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>6371</td>\n",
       "      <td>hostages</td>\n",
       "      <td>Germany</td>\n",
       "      <td>@banditregina I also loved the episode 'Bang' ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>9388</td>\n",
       "      <td>survived</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Survived Spanish!! @ Sweet Ritual https://t.co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>5366</td>\n",
       "      <td>fire%20truck</td>\n",
       "      <td>United States</td>\n",
       "      <td>#RFP: Fire Truck Service Body for F-450 (Fire ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>479</td>\n",
       "      <td>armageddon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So the Ahamedis think the Messiah had already ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>4324</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Let it be gone away like a dust in the wind .....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows ร 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            keyword       location  \\\n",
       "0     2115        catastrophe        Florida   \n",
       "1     9232  suicide%20bombing            NaN   \n",
       "2     8131            rescued        Ireland   \n",
       "3     8236               riot        Seattle   \n",
       "4     9822             trauma            NaN   \n",
       "...    ...                ...            ...   \n",
       "5324  6371           hostages        Germany   \n",
       "5325  9388           survived     Austin, TX   \n",
       "5326  5366       fire%20truck  United States   \n",
       "5327   479         armageddon            NaN   \n",
       "5328  4324       dust%20storm            NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     @deb117 7/30 that catastrophe man opens school...       0  \n",
       "1     meek mill should join isis since he loves suic...       0  \n",
       "2     Three beached whales rescued in Kerry - http:/...       1  \n",
       "3     Southeast Dirt Riot Series Crowns Champions:  ...       0  \n",
       "4     Hiroshima: They told me to paint my story: Eig...       1  \n",
       "...                                                 ...     ...  \n",
       "5324  @banditregina I also loved the episode 'Bang' ...       0  \n",
       "5325  Survived Spanish!! @ Sweet Ritual https://t.co...       0  \n",
       "5326  #RFP: Fire Truck Service Body for F-450 (Fire ...       0  \n",
       "5327  So the Ahamedis think the Messiah had already ...       0  \n",
       "5328  Let it be gone away like a dust in the wind .....       1  \n",
       "\n",
       "[5329 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test_x = pd.read_csv(\"test_x.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde31fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example: URL removal\n",
    "def remove_url(data):\n",
    "    re1 = r\"http://\\S+|www\\.\\S+\"\n",
    "    re2 = r\"https://\\S+|www\\.\\S+\"\n",
    "    url_clean= re.compile(\"(%s|%s)\" % (re1, re2))\n",
    "    data=url_clean.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "# !!! YOUR TASK 1 (4 point): add TWO more preprocessing steps from the above list\n",
    "# you can choose two functions to implement\n",
    "def to_lowercase(data):\n",
    "    # your implementation\n",
    "    \n",
    "    return data.lower()\n",
    "\n",
    "def remove_hashtag(data):\n",
    "    # your implementation\n",
    "    # Define a regular expression pattern to match hashtags\n",
    "    hashtag_pattern =r'#\\w+'\n",
    "    \n",
    "    # Use re.sub() to replace hashtags with an empty string\n",
    "    data = re.sub(hashtag_pattern, '', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee3e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removal:  #RFP: Fire Truck Service Body for F-450 (Fire fighting rescue &amp; safety equipment Transporta... http://t.co/8GtRvEcE1N\n",
      "after removal:  : fire truck service body for f-450 (fire fighting rescue &amp; safety equipment transporta... \n"
     ]
    }
   ],
   "source": [
    "#see the preprocess result\n",
    "print('before removal: ', train.iloc[5326]['text'])\n",
    "\n",
    "#train['text'] = train['text'].apply(remove_url)\n",
    "#test_x['text'] = test_x['text'].apply(remove_url)\n",
    "train['text'] = train['text'].apply(lambda x: remove_url(to_lowercase(remove_hashtag(x))))\n",
    "test_x['text'] = test_x['text'].apply(lambda x: remove_url(to_lowercase(remove_hashtag(x))))\n",
    "\n",
    "print('after removal: ', train.iloc[5326]['text'])\n",
    "# proprocess both test and train data, result looks good , 5326 of test lower casem, no # anymore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d36cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  5000\n",
      "(5329, 5000)\n",
      "(2284, 5000)\n"
     ]
    }
   ],
   "source": [
    "# This package generates bag of words feature for each text\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# doc for this function: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.9, min_df=2, max_features=5000) \n",
    "\n",
    "# use training data to construct vocabulary and turn training text into BOW vectors\n",
    "train_bow = vectorizer.fit_transform(train['text'])\n",
    "vocab = vectorizer.vocabulary_.copy()\n",
    "# You can change the vocab size by tuning \"max_df\", \"min_df\" or \"max_features\" in \"CountVectorizer\"\n",
    "print('vocab size: ', len(vocab))  \n",
    "print (train_bow.shape) # num_train_text * vocab_size\n",
    "\n",
    "# !!! YOUR TASK 2 (1 point): transform test text into BOW vectors using vectorizer\n",
    "test_bow = vectorizer.transform(test_x['text'])\n",
    "print (test_bow.shape) # num_train_text * vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f7d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Use basic ML models on BOW features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split given training data into training and validation set\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_bow, train['target'], test_size=0.3, random_state=11)\n",
    "\n",
    "LR = RandomForestClassifier()\n",
    "#tuning the parameters of random forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#Grid search\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331455ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training recall:  0.9146417445482866\n",
      "validation recall:  0.6394658753709199\n"
     ]
    }
   ],
   "source": [
    "LR = RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_leaf=1, min_samples_split=10)\n",
    "LR.fit(X_train.toarray(),y_train)\n",
    "print('training recall: ', recall_score(y_train, LR.predict(X_train.toarray())))\n",
    "print('validation recall: ', recall_score(y_val, LR.predict(X_val.toarray())))\n",
    "test_y_pred = LR.predict(test_bow)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_x['id'],\n",
    "    'target': test_y_pred\n",
    "})\n",
    "predictions_df.to_csv(\"test_y_pred.csv\", index=False)\n",
    "\n",
    "\n",
    "# Optional: apply more advanced model selection technique\n",
    "# e.g., sklearn.model_selection.GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6238b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526856, 795450)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train word embeddings using gensim library\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_docs = [train['text'].iloc[i].split(' ') for i in range(len(train['text']))] # gensim only takes list of word list as `sentences`\n",
    "\n",
    "w2v_model = Word2Vec(sentences=train_docs, vector_size=100, window=5, negative=20, min_count=2, workers=1)\n",
    "w2v_model.train(train_docs, total_examples=len(train_docs), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c020c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b459cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example words in vocab:  ['', 'the', 'a', 'to', 'in', 'of', 'and', 'i', 'is', 'for', 'on', 'you', '-', 'my', 'that', 'with', 'by', 'at', 'it', 'this', 'are', 'from', 'be', 'was', 'have', 'like', 'as', 'just', 'up', 'so', 'but', '&amp;', 'your', 'not', 'me', 'will', 'after', 'an', 'has', 'out', \"i'm\", 'when', 'get', 'all', '??', 'fire', 'if', 'new', 'via', 'no', 'more', 'we', 'or', 'about', 'he', 'how', 'they', 'over', 'what', 'into', \"it's\", 'now', 'people', 'do', 'one', '...', 'who', 'been', 'can', '|', \"don't\", 'than', '2', 'emergency', 'some', 'there', 'his', 'police', 'burning', 'disaster', 'still', 'her', 'body', 'would', ':', 'love', 'suicide', 'got', 'video', 'going', 'why', 'man', 'back', 'had', 'killed', 'see', 'first', 'off', 'california', 'us', 'buildings', 'storm', 'full', '3', 'families', \"can't\", '????', 'car', 'know', 'nuclear', 'were', 'two', 'train', 'our', 'day', 'crash', 'time', 'say', 'go', 'think', '@youtube', 'them', 'news', 'last', 'could', 'many', 'their', 'bomb', 'being', 'down', 'only', 'its', 'may', 'u', 'rt', 'watch', 'attack', 'too', 'am', 'world', 'war', 'fires', 'dead', 'want', 'today', 'did', 'because', 'really', 'then', 'fatal', 'take', 'good', 'bombing', 'need', 'way', \"you're\", '\\x89รป_', 'even', 'years', '??????', 'accident', 'mass', 'should', 'make', 'work', 'those', 'another', 'never', 'death', 'much', 'right', 'army', 'hit', 'help', 'school', '.', 'bomber', 'home', 'northern', '\\n', 'im', 'life', 'most', 'said', 'look', 'atomic', 'here', 'near', 'hiroshima', 'collapse', 'feel', 'obama', 'every', 'homes', 'these', 'forest', 'any', 'old', 'wildfire', 'wreck']\n",
      "learned embedding of bomb:  [-0.98935735  0.04641643  0.6349603   0.26025766  0.14935167 -0.77661884\n",
      " -0.60027444  0.13670002  0.1741676  -0.29033145  0.4876115  -0.5165124\n",
      " -0.46907207 -0.708577   -0.53960305 -0.8222442  -0.14342004 -0.17490762\n",
      " -0.07434633 -0.60778797  0.15254925  0.7618108   0.5398134   0.08525655\n",
      "  0.03242154 -0.43528262 -0.6774115  -0.57732666 -0.6440236  -0.03035907\n",
      "  0.44565037  0.72674996  0.10348924 -0.529801   -0.1560153  -0.30183148\n",
      "  0.16533743 -0.84584635  0.41860914 -1.2755337   0.17907035 -0.46332744\n",
      " -0.43415287 -0.58551854  0.8992186   0.02843775 -0.45110607  0.32485762\n",
      " -0.00381751  0.5639665   0.12842065 -0.80930495 -0.13687916 -0.395093\n",
      " -0.30620942 -0.41212282  0.06187369 -0.6212089  -0.52742803  0.1805276\n",
      "  0.12202628  0.03132014 -0.1347765  -0.20232531 -0.23635618 -0.23204547\n",
      "  0.5380832  -0.19691925  0.00285106  0.97148037 -0.41990188  0.57524747\n",
      "  0.04784777  0.6846776   0.09692425  0.2570454   0.49215677 -0.21649571\n",
      " -0.5828695  -0.6506718  -0.70743775 -0.58408964  0.04840912  0.5430017\n",
      "  0.36502966 -0.6861851  -0.57022953  0.3450593  -0.5400295   0.63360435\n",
      "  0.2518819   0.4145377   1.1985874   0.66578484  1.1679196   0.37450182\n",
      " -0.14210732 -0.7674436  -0.07411598 -0.36716476]\n",
      "detonated: 0.9830370545387268\n",
      "bomber: 0.9691240191459656\n",
      "turkey: 0.9602714776992798\n",
      "suicide: 0.9564100503921509\n",
      "pkk: 0.9482672810554504\n",
      "bombing: 0.9413579106330872\n",
      "army: 0.9408153295516968\n",
      "old: 0.9327422976493835\n",
      "16yr: 0.9276955127716064\n",
      "saudi: 0.9273166060447693\n",
      "example words in vocab:  ['', 'the', 'a', 'to', 'in', 'of', 'and', 'i', 'is', 'for', 'on', 'you', '-', 'my', 'that', 'with', 'by', 'at', 'it', 'this', 'are', 'from', 'be', 'was', 'have', 'like', 'as', 'just', 'up', 'so', 'but', 'your', '&amp;', 'not', 'me', 'will', 'after', 'an', 'has', 'out', \"i'm\", 'when', 'get', 'all', '??', 'fire', 'if', 'new', 'via', 'no', 'more', 'we', 'or', 'about', 'he', 'how', 'they', 'over', 'what', 'into', \"it's\", 'now', 'people', 'do', 'one', '...', 'who', 'been', 'can', \"don't\", '|', '2', 'than', 'emergency', 'some', 'there', 'his', 'police', 'burning', 'disaster', 'still', 'her', 'body', 'would', ':', 'love', 'got', 'video', 'suicide', 'going', 'why', 'man', 'back', 'had', 'first', 'see', 'killed', 'off', 'california', 'buildings', 'us', 'storm', 'families', \"can't\", 'full', '3', '????', 'car', 'know', 'were', 'two', 'nuclear', 'train', 'our', 'day', 'crash', 'say', 'go', 'time', 'them', 'think', '@youtube', 'news', 'last', 'many', 'could', 'their', 'bomb', 'being', 'down', 'only', 'its', 'u', 'may', 'watch', 'rt', 'attack', 'too', 'am', 'war', 'fires', 'world', 'dead', 'want', 'today', 'because', 'fatal', 'then', 'really', 'did', 'good', 'take', 'bombing', 'need', \"you're\", '\\x89รป_', 'way', 'even', '??????', 'years', 'should', 'accident', 'mass', 'make', 'another', 'work', 'those', 'never', 'much', 'death', 'school', 'right', 'help', 'army', '.', 'hit', 'bomber', 'northern', '\\n', 'said', 'most', 'home', 'life', 'im', 'look', 'atomic', 'here', 'near', 'hiroshima', 'collapse', 'forest', 'homes', 'wildfire', 'every', 'feel', 'these', \"that's\", 'old', 'any', 'obama']\n",
      "learned embedding of bomb:  [ 0.75857675 -0.3329341  -0.39605424 -0.09660213  0.50502956  0.49554953\n",
      " -0.85577595 -0.47128898  0.4864998  -1.1433709  -0.1579673   0.00362016\n",
      "  0.7715398  -0.5613782   0.15422118 -0.8473428  -0.6426236  -0.2719968\n",
      "  0.07386146  0.9381394  -0.5540957  -0.25134957  0.0768607   0.6787704\n",
      "  0.37656897 -0.08052386  0.81868106 -0.9581858  -1.1895201  -0.4577665\n",
      "  0.1270185  -0.2444651  -0.04878654  0.4915388  -0.35543686 -0.329874\n",
      " -0.32915863 -0.49020365  0.6029539   0.04393579 -0.39945355  0.6183655\n",
      " -0.01931076  1.4754146  -0.32434526 -0.50365657 -0.7840062  -0.40715903\n",
      "  0.3283171   0.44720897 -0.04424508 -0.37283197  0.2258882   0.08376358\n",
      "  0.28423372 -1.0120598   0.04759936  0.301082   -0.6078535   0.23778796\n",
      " -0.83108246  1.2464539  -0.02576076 -0.1538588  -0.25996923 -0.2558147\n",
      " -0.18062896  0.06451588 -0.09585061  1.0058988   0.29609594  0.2677022\n",
      " -0.07237327  0.89396465  0.23621224 -0.05590812  1.2670789   0.1799927\n",
      " -0.5265937  -0.01085144 -0.45152843 -0.53635263 -0.10153535 -0.27812728\n",
      " -0.7711234  -0.44483232 -0.1957526  -0.0545864  -0.52631193  0.34010223\n",
      "  0.12058956  0.03379781  0.62622446  0.76832455  0.40991518  0.7960504\n",
      " -0.39963776  0.18369898 -0.49126524  0.47470662  0.33661178  0.36991718\n",
      " -0.07827248 -1.0465631  -0.10053823 -0.23041241  0.5994192   0.12100735\n",
      " -0.22319421 -0.6976157  -0.057127   -0.6805218  -0.7328176  -0.27677414\n",
      " -0.6446762  -0.07098237  0.26608545 -0.9666713  -0.20574382 -0.772376\n",
      "  0.4576559   0.23924772  0.48219675  0.17083745 -0.24859352  0.82401395\n",
      "  0.1490038   0.5614976  -0.44120312  0.47304648  0.70211697  0.35731775\n",
      " -0.23256463 -1.062275   -0.7311373   0.5829148  -1.3568856  -0.19143748\n",
      " -0.12090299 -0.28704432  0.7246507  -0.26879868  0.52440864 -0.18623362\n",
      " -0.5117894  -0.4354342   0.9239715  -0.7469662   0.18168491  0.30098805\n",
      "  0.21814516 -0.80170554 -0.2981305  -0.29521087  0.04135555 -0.44233093\n",
      "  0.9484526   0.521516   -0.29157513  0.07404469 -0.03711013  0.03846785\n",
      " -0.57352996 -0.10739749  0.04564977  0.8060869   0.2774422  -0.39770275\n",
      " -0.19550768  0.96443963 -0.67502433 -0.04761606  0.44885916 -0.40767393\n",
      " -0.66507304  0.02994178  0.8263774   0.43247482 -0.12555392  0.60010904\n",
      " -0.16735254  0.4705128  -0.15355064  1.1022553   0.1367064   1.0148172\n",
      "  0.00444098  1.4054114   0.4818463   0.28939715  0.24491239  0.16568582\n",
      " -1.0641159   0.15830365  1.1287353   0.6718386   0.42672396  0.51151437\n",
      " -0.8508881  -0.14109117]\n"
     ]
    }
   ],
   "source": [
    "# check the learned word embeddings\n",
    "\n",
    "# vocabulary\n",
    "w2v_vocav = w2v_model.wv.index_to_key\n",
    "print('example words in vocab: ', [w2v_vocav[i] for i in range(200)])\n",
    "\n",
    "# example embedding\n",
    "example_word = 'bomb'\n",
    "word_vec = w2v_model.wv[example_word]\n",
    "print('learned embedding of bomb: ', word_vec)\n",
    "\n",
    "# !!! YOUR TASK (2 point): show the top-10 words most similar to 'bomb'\n",
    "# you may calculate manually, or refer to gensim function: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "similar_words = w2v_model.wv.most_similar('bomb', topn=10)\n",
    "\n",
    "# Print the similar words\n",
    "for word, score in similar_words:\n",
    "    print(f'{word}: {score}')\n",
    "\n",
    "# !!! Your TASK (1 point): tune the arguments of Word2Vec to obtain different word embeddings \n",
    "# \n",
    "# Tune the parameters of Word2Vec, I will come back to this task later so that the work folw is not interrupted now.\n",
    "w2v_model = Word2Vec(sentences=train_docs, vector_size=200, window=10, negative=20, min_count=3, workers=1)\n",
    "w2v_model.train(train_docs, total_examples=len(train_docs), epochs=20)\n",
    "w2v_vocav = w2v_model.wv.index_to_key\n",
    "print('example words in vocab: ', [w2v_vocav[i] for i in range(200)])\n",
    "example_word = 'bomb'\n",
    "word_vec = w2v_model.wv[example_word]\n",
    "print('learned embedding of bomb: ', word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8599fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec shape:  (5329, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyuwu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/siyuwu/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test word2vec shape:  (2284, 200)\n",
      "training word2vec shape:  (5329, 200)\n",
      "test word2vec shape:  (2284, 200)\n",
      "Validation accuracy: 76.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyuwu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Use basic ML models on word embeddings\n",
    "\n",
    "# !!! YOUR TASK (2 points): turn word embedding to doc embedding\n",
    "# Choice 1 (as shown below): manualy take the average of word embeddings in each doc\n",
    "# Choise 2: gensim doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html \n",
    "def doc2vec(docs, w2v_model):\n",
    "    w2v = []\n",
    "    for doc in docs:\n",
    "        word_embs = [w2v_model.wv[w] for w in doc if w in w2v_model.wv] # get a list of word embeddings\n",
    "        \n",
    "        # TODO: take (weighted) average to get doc embedding\n",
    "        doc_emb = np.mean(np.array(word_embs), axis=0) \n",
    "        \n",
    "        # filling zeros for empty doc\n",
    "        if len(word_embs) == 0:\n",
    "            doc_emb = [0 for i in range(w2v_model.vector_size)]\n",
    "        \n",
    "        w2v.append(doc_emb)\n",
    "    \n",
    "    return np.array(w2v)\n",
    "        \n",
    "# transform training tweets into embeddings\n",
    "train_w2v = doc2vec(train_docs, w2v_model)\n",
    "print('training word2vec shape: ', train_w2v.shape)\n",
    "\n",
    "test_w2v = doc2vec(test_x['text'], w2v_model) \n",
    "print('test word2vec shape: ', test_w2v.shape)\n",
    "\n",
    "        \n",
    "# transform training tweets into embeddings\n",
    "train_w2v = doc2vec(train_docs, w2v_model)\n",
    "print('training word2vec shape: ', train_w2v.shape)\n",
    "\n",
    "test_w2v = doc2vec(test_x['text'], w2v_model) \n",
    "print('test word2vec shape: ', test_w2v.shape)\n",
    "\n",
    "# !!! YOUR TASK (2 point): apply a basic ML model on word embeddings\n",
    "# you may consider tune the word2vec model to gain better embeddings (refer to gensim document about argument details: https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_w2v, train['target'], test_size=0.3, random_state=11)\n",
    "# Initialize and train a logistic regression classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "test_y_pred = clf.predict(test_w2v)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_x['id'],\n",
    "    'target': test_y_pred\n",
    "})\n",
    "predictions_df.to_csv(\"test_y_pred.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e70b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5366e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5acb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
