{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyuwu/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/siyuwu/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 6): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: /Users/siyuwu/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/siyuwu/anaconda3/lib/python3.11/site-packages/torch/lib/libc10.dylib\n",
      " in /Users/siyuwu/anaconda3/lib/python3.11/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=100, out_features=128), # Random noise dimension to 128\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 3) # Output features to match tabular data columns\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features=512), # Input features to match tabular data columns\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The datasets needed are computed by the `ComputeGluon.py` script in PseudoData\n",
    "filename1='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-3.dat'\n",
    "filename2='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-4.dat'\n",
    "filename3='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-5.dat'\n",
    "filename4='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-6.dat'\n",
    "\n",
    "# Headers to skip\n",
    "lines_to_skip = 5\n",
    "\n",
    "# Defining the columns (cv = central value, sd = standard deviation)\n",
    "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
    "\n",
    "# Loading data from txt file\n",
    "# Change filename1 to another filename for data that extends to lower x\n",
    "# (see exercises at the bottom of this notebook)\n",
    "df = pd.read_csv(filename1,\n",
    "                 sep=\"\\s+\",\n",
    "                 skiprows=lines_to_skip,\n",
    "                 usecols=[0,1,2],\n",
    "                 names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise as input for the generator\n",
    "batch_size = 20\n",
    "noise = torch.randn(batch_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Binary Cross Entropy Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.dataframe.iloc[idx].values, dtype=torch.float)\n",
    "dataset = TabularDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/16] Loss_D: 1.5692 Loss_G: 0.7886 D(x): 0.3853 D(G(z)): 0.4546 / 0.4546\n",
      "[1/100][0/16] Loss_D: 1.5681 Loss_G: 0.7880 D(x): 0.3858 D(G(z)): 0.4549 / 0.4549\n",
      "[2/100][0/16] Loss_D: 1.5640 Loss_G: 0.7890 D(x): 0.3869 D(G(z)): 0.4544 / 0.4544\n",
      "[3/100][0/16] Loss_D: 1.5646 Loss_G: 0.7879 D(x): 0.3868 D(G(z)): 0.4549 / 0.4549\n",
      "[4/100][0/16] Loss_D: 1.5577 Loss_G: 0.7892 D(x): 0.3898 D(G(z)): 0.4544 / 0.4544\n",
      "[5/100][0/16] Loss_D: 1.5846 Loss_G: 0.7880 D(x): 0.3789 D(G(z)): 0.4549 / 0.4549\n",
      "[6/100][0/16] Loss_D: 1.5692 Loss_G: 0.7890 D(x): 0.3853 D(G(z)): 0.4544 / 0.4544\n",
      "[7/100][0/16] Loss_D: 1.5872 Loss_G: 0.7884 D(x): 0.3780 D(G(z)): 0.4547 / 0.4547\n",
      "[8/100][0/16] Loss_D: 1.5518 Loss_G: 0.7882 D(x): 0.3917 D(G(z)): 0.4548 / 0.4548\n",
      "[9/100][0/16] Loss_D: 1.5633 Loss_G: 0.7877 D(x): 0.3874 D(G(z)): 0.4550 / 0.4550\n",
      "[10/100][0/16] Loss_D: 1.5870 Loss_G: 0.7894 D(x): 0.3782 D(G(z)): 0.4542 / 0.4542\n",
      "[11/100][0/16] Loss_D: 1.5381 Loss_G: 0.7880 D(x): 0.3978 D(G(z)): 0.4548 / 0.4548\n",
      "[12/100][0/16] Loss_D: 1.5745 Loss_G: 0.7881 D(x): 0.3834 D(G(z)): 0.4548 / 0.4548\n",
      "[13/100][0/16] Loss_D: 1.5544 Loss_G: 0.7888 D(x): 0.3907 D(G(z)): 0.4545 / 0.4545\n",
      "[14/100][0/16] Loss_D: 1.5460 Loss_G: 0.7885 D(x): 0.3936 D(G(z)): 0.4546 / 0.4546\n",
      "[15/100][0/16] Loss_D: 1.5634 Loss_G: 0.7882 D(x): 0.3875 D(G(z)): 0.4548 / 0.4548\n",
      "[16/100][0/16] Loss_D: 1.5928 Loss_G: 0.7874 D(x): 0.3764 D(G(z)): 0.4552 / 0.4552\n",
      "[17/100][0/16] Loss_D: 1.5935 Loss_G: 0.7885 D(x): 0.3761 D(G(z)): 0.4547 / 0.4547\n",
      "[18/100][0/16] Loss_D: 1.5778 Loss_G: 0.7881 D(x): 0.3821 D(G(z)): 0.4548 / 0.4548\n",
      "[19/100][0/16] Loss_D: 1.5750 Loss_G: 0.7873 D(x): 0.3834 D(G(z)): 0.4551 / 0.4551\n",
      "[20/100][0/16] Loss_D: 1.5662 Loss_G: 0.7872 D(x): 0.3865 D(G(z)): 0.4552 / 0.4552\n",
      "[21/100][0/16] Loss_D: 1.5686 Loss_G: 0.7887 D(x): 0.3857 D(G(z)): 0.4545 / 0.4545\n",
      "[22/100][0/16] Loss_D: 1.5792 Loss_G: 0.7882 D(x): 0.3814 D(G(z)): 0.4548 / 0.4548\n",
      "[23/100][0/16] Loss_D: 1.5694 Loss_G: 0.7884 D(x): 0.3854 D(G(z)): 0.4547 / 0.4547\n",
      "[24/100][0/16] Loss_D: 1.5606 Loss_G: 0.7881 D(x): 0.3885 D(G(z)): 0.4548 / 0.4548\n",
      "[25/100][0/16] Loss_D: 1.5653 Loss_G: 0.7882 D(x): 0.3863 D(G(z)): 0.4547 / 0.4547\n",
      "[26/100][0/16] Loss_D: 1.5539 Loss_G: 0.7873 D(x): 0.3918 D(G(z)): 0.4552 / 0.4552\n",
      "[27/100][0/16] Loss_D: 1.5793 Loss_G: 0.7895 D(x): 0.3807 D(G(z)): 0.4542 / 0.4542\n",
      "[28/100][0/16] Loss_D: 1.5494 Loss_G: 0.7884 D(x): 0.3925 D(G(z)): 0.4547 / 0.4547\n",
      "[29/100][0/16] Loss_D: 1.5853 Loss_G: 0.7878 D(x): 0.3789 D(G(z)): 0.4549 / 0.4549\n",
      "[30/100][0/16] Loss_D: 1.5551 Loss_G: 0.7885 D(x): 0.3906 D(G(z)): 0.4547 / 0.4547\n",
      "[31/100][0/16] Loss_D: 1.5889 Loss_G: 0.7888 D(x): 0.3777 D(G(z)): 0.4545 / 0.4545\n",
      "[32/100][0/16] Loss_D: 1.5820 Loss_G: 0.7876 D(x): 0.3806 D(G(z)): 0.4551 / 0.4551\n",
      "[33/100][0/16] Loss_D: 1.5943 Loss_G: 0.7883 D(x): 0.3755 D(G(z)): 0.4547 / 0.4547\n",
      "[34/100][0/16] Loss_D: 1.5684 Loss_G: 0.7878 D(x): 0.3857 D(G(z)): 0.4549 / 0.4549\n",
      "[35/100][0/16] Loss_D: 1.5594 Loss_G: 0.7888 D(x): 0.3890 D(G(z)): 0.4545 / 0.4545\n",
      "[36/100][0/16] Loss_D: 1.5659 Loss_G: 0.7895 D(x): 0.3857 D(G(z)): 0.4542 / 0.4542\n",
      "[37/100][0/16] Loss_D: 1.5867 Loss_G: 0.7891 D(x): 0.3785 D(G(z)): 0.4544 / 0.4544\n",
      "[38/100][0/16] Loss_D: 1.5689 Loss_G: 0.7889 D(x): 0.3850 D(G(z)): 0.4544 / 0.4544\n",
      "[39/100][0/16] Loss_D: 1.5552 Loss_G: 0.7878 D(x): 0.3907 D(G(z)): 0.4549 / 0.4549\n",
      "[40/100][0/16] Loss_D: 1.5826 Loss_G: 0.7889 D(x): 0.3797 D(G(z)): 0.4544 / 0.4544\n",
      "[41/100][0/16] Loss_D: 1.5641 Loss_G: 0.7879 D(x): 0.3876 D(G(z)): 0.4549 / 0.4549\n",
      "[42/100][0/16] Loss_D: 1.5776 Loss_G: 0.7886 D(x): 0.3816 D(G(z)): 0.4546 / 0.4546\n",
      "[43/100][0/16] Loss_D: 1.5586 Loss_G: 0.7891 D(x): 0.3890 D(G(z)): 0.4544 / 0.4544\n",
      "[44/100][0/16] Loss_D: 1.5783 Loss_G: 0.7871 D(x): 0.3820 D(G(z)): 0.4553 / 0.4553\n",
      "[45/100][0/16] Loss_D: 1.5533 Loss_G: 0.7891 D(x): 0.3915 D(G(z)): 0.4544 / 0.4544\n",
      "[46/100][0/16] Loss_D: 1.5503 Loss_G: 0.7874 D(x): 0.3928 D(G(z)): 0.4552 / 0.4552\n",
      "[47/100][0/16] Loss_D: 1.5710 Loss_G: 0.7878 D(x): 0.3850 D(G(z)): 0.4549 / 0.4549\n",
      "[48/100][0/16] Loss_D: 1.5381 Loss_G: 0.7894 D(x): 0.3972 D(G(z)): 0.4542 / 0.4542\n",
      "[49/100][0/16] Loss_D: 1.5761 Loss_G: 0.7883 D(x): 0.3821 D(G(z)): 0.4547 / 0.4547\n",
      "[50/100][0/16] Loss_D: 1.5729 Loss_G: 0.7881 D(x): 0.3841 D(G(z)): 0.4548 / 0.4548\n",
      "[51/100][0/16] Loss_D: 1.5717 Loss_G: 0.7880 D(x): 0.3841 D(G(z)): 0.4548 / 0.4548\n",
      "[52/100][0/16] Loss_D: 1.5520 Loss_G: 0.7876 D(x): 0.3921 D(G(z)): 0.4550 / 0.4550\n",
      "[53/100][0/16] Loss_D: 1.5980 Loss_G: 0.7881 D(x): 0.3743 D(G(z)): 0.4548 / 0.4548\n",
      "[54/100][0/16] Loss_D: 1.5850 Loss_G: 0.7875 D(x): 0.3795 D(G(z)): 0.4551 / 0.4551\n",
      "[55/100][0/16] Loss_D: 1.5556 Loss_G: 0.7891 D(x): 0.3904 D(G(z)): 0.4543 / 0.4543\n",
      "[56/100][0/16] Loss_D: 1.5692 Loss_G: 0.7877 D(x): 0.3853 D(G(z)): 0.4550 / 0.4550\n",
      "[57/100][0/16] Loss_D: 1.5598 Loss_G: 0.7864 D(x): 0.3895 D(G(z)): 0.4556 / 0.4556\n",
      "[58/100][0/16] Loss_D: 1.5955 Loss_G: 0.7880 D(x): 0.3754 D(G(z)): 0.4549 / 0.4549\n",
      "[59/100][0/16] Loss_D: 1.5729 Loss_G: 0.7875 D(x): 0.3840 D(G(z)): 0.4551 / 0.4551\n",
      "[60/100][0/16] Loss_D: 1.5574 Loss_G: 0.7879 D(x): 0.3891 D(G(z)): 0.4549 / 0.4549\n",
      "[61/100][0/16] Loss_D: 1.5536 Loss_G: 0.7887 D(x): 0.3915 D(G(z)): 0.4545 / 0.4545\n",
      "[62/100][0/16] Loss_D: 1.5882 Loss_G: 0.7882 D(x): 0.3780 D(G(z)): 0.4547 / 0.4547\n",
      "[63/100][0/16] Loss_D: 1.5691 Loss_G: 0.7883 D(x): 0.3852 D(G(z)): 0.4547 / 0.4547\n",
      "[64/100][0/16] Loss_D: 1.6046 Loss_G: 0.7886 D(x): 0.3713 D(G(z)): 0.4546 / 0.4546\n",
      "[65/100][0/16] Loss_D: 1.5597 Loss_G: 0.7889 D(x): 0.3883 D(G(z)): 0.4545 / 0.4545\n",
      "[66/100][0/16] Loss_D: 1.5738 Loss_G: 0.7882 D(x): 0.3835 D(G(z)): 0.4548 / 0.4548\n",
      "[67/100][0/16] Loss_D: 1.5645 Loss_G: 0.7880 D(x): 0.3874 D(G(z)): 0.4549 / 0.4549\n",
      "[68/100][0/16] Loss_D: 1.5851 Loss_G: 0.7901 D(x): 0.3788 D(G(z)): 0.4539 / 0.4539\n",
      "[69/100][0/16] Loss_D: 1.5825 Loss_G: 0.7885 D(x): 0.3799 D(G(z)): 0.4546 / 0.4546\n",
      "[70/100][0/16] Loss_D: 1.5588 Loss_G: 0.7881 D(x): 0.3894 D(G(z)): 0.4548 / 0.4548\n",
      "[71/100][0/16] Loss_D: 1.5439 Loss_G: 0.7879 D(x): 0.3956 D(G(z)): 0.4549 / 0.4549\n",
      "[72/100][0/16] Loss_D: 1.5895 Loss_G: 0.7887 D(x): 0.3775 D(G(z)): 0.4545 / 0.4545\n",
      "[73/100][0/16] Loss_D: 1.5514 Loss_G: 0.7894 D(x): 0.3919 D(G(z)): 0.4542 / 0.4542\n",
      "[74/100][0/16] Loss_D: 1.5825 Loss_G: 0.7884 D(x): 0.3802 D(G(z)): 0.4547 / 0.4547\n",
      "[75/100][0/16] Loss_D: 1.5667 Loss_G: 0.7883 D(x): 0.3864 D(G(z)): 0.4547 / 0.4547\n",
      "[76/100][0/16] Loss_D: 1.5631 Loss_G: 0.7887 D(x): 0.3876 D(G(z)): 0.4545 / 0.4545\n",
      "[77/100][0/16] Loss_D: 1.5651 Loss_G: 0.7883 D(x): 0.3869 D(G(z)): 0.4547 / 0.4547\n",
      "[78/100][0/16] Loss_D: 1.5490 Loss_G: 0.7891 D(x): 0.3932 D(G(z)): 0.4543 / 0.4543\n",
      "[79/100][0/16] Loss_D: 1.5814 Loss_G: 0.7869 D(x): 0.3813 D(G(z)): 0.4554 / 0.4554\n",
      "[80/100][0/16] Loss_D: 1.5846 Loss_G: 0.7883 D(x): 0.3796 D(G(z)): 0.4547 / 0.4547\n",
      "[81/100][0/16] Loss_D: 1.5635 Loss_G: 0.7875 D(x): 0.3878 D(G(z)): 0.4551 / 0.4551\n",
      "[82/100][0/16] Loss_D: 1.5508 Loss_G: 0.7885 D(x): 0.3927 D(G(z)): 0.4546 / 0.4546\n",
      "[83/100][0/16] Loss_D: 1.5737 Loss_G: 0.7879 D(x): 0.3838 D(G(z)): 0.4549 / 0.4549\n",
      "[84/100][0/16] Loss_D: 1.5728 Loss_G: 0.7889 D(x): 0.3836 D(G(z)): 0.4545 / 0.4545\n",
      "[85/100][0/16] Loss_D: 1.5666 Loss_G: 0.7885 D(x): 0.3862 D(G(z)): 0.4546 / 0.4546\n",
      "[86/100][0/16] Loss_D: 1.5798 Loss_G: 0.7893 D(x): 0.3807 D(G(z)): 0.4543 / 0.4543\n",
      "[87/100][0/16] Loss_D: 1.5999 Loss_G: 0.7878 D(x): 0.3734 D(G(z)): 0.4550 / 0.4550\n",
      "[88/100][0/16] Loss_D: 1.6005 Loss_G: 0.7879 D(x): 0.3732 D(G(z)): 0.4549 / 0.4549\n",
      "[89/100][0/16] Loss_D: 1.5879 Loss_G: 0.7881 D(x): 0.3784 D(G(z)): 0.4548 / 0.4548\n",
      "[90/100][0/16] Loss_D: 1.5339 Loss_G: 0.7873 D(x): 0.3992 D(G(z)): 0.4551 / 0.4551\n",
      "[91/100][0/16] Loss_D: 1.5603 Loss_G: 0.7890 D(x): 0.3887 D(G(z)): 0.4544 / 0.4544\n",
      "[92/100][0/16] Loss_D: 1.5476 Loss_G: 0.7892 D(x): 0.3925 D(G(z)): 0.4543 / 0.4543\n",
      "[93/100][0/16] Loss_D: 1.5697 Loss_G: 0.7880 D(x): 0.3848 D(G(z)): 0.4548 / 0.4548\n",
      "[94/100][0/16] Loss_D: 1.5756 Loss_G: 0.7885 D(x): 0.3829 D(G(z)): 0.4546 / 0.4546\n",
      "[95/100][0/16] Loss_D: 1.5895 Loss_G: 0.7870 D(x): 0.3775 D(G(z)): 0.4553 / 0.4553\n",
      "[96/100][0/16] Loss_D: 1.5779 Loss_G: 0.7885 D(x): 0.3816 D(G(z)): 0.4546 / 0.4546\n",
      "[97/100][0/16] Loss_D: 1.5973 Loss_G: 0.7888 D(x): 0.3743 D(G(z)): 0.4545 / 0.4545\n",
      "[98/100][0/16] Loss_D: 1.5714 Loss_G: 0.7883 D(x): 0.3844 D(G(z)): 0.4547 / 0.4547\n",
      "[99/100][0/16] Loss_D: 1.5773 Loss_G: 0.7877 D(x): 0.3823 D(G(z)): 0.4550 / 0.4550\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Update Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        discriminator.zero_grad()\n",
    "        # Train with real data\n",
    "        real_data = data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        label = torch.full((batch_size,), 1, dtype=torch.float, device=device) # Real data label = 1\n",
    "        output = discriminator(real_data).view(-1)\n",
    "        lossD_real = criterion(output, label)\n",
    "        lossD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with fake data\n",
    "        noise = torch.randn(batch_size, 100, device=device)\n",
    "        fake_data = generator(noise)\n",
    "        label.fill_(0) # Fake data label = 0\n",
    "        output = discriminator(fake_data.detach()).view(-1)\n",
    "        lossD_fake = criterion(output, label)\n",
    "        lossD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Update Generator: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        label.fill_(1) # Fake labels are real for generator cost\n",
    "        output = discriminator(fake_data).view(-1)\n",
    "        lossG = criterion(output, label)\n",
    "        lossG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                  f'Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f} '\n",
    "                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100  # Number of synthetic samples want to generate\n",
    "noise_dim = 100\n",
    "noise = torch.randn(num_samples, noise_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    generator.eval()  # Set the generator to evaluation mode\n",
    "    synthetic_data = generator(noise)\n",
    "    generator.train()  # Set it back to train mode if you're continuing training later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# real data had column names: 'x', 'gluon_cv', 'gluon_sd'\n",
    "column_names = ['x', 'gluon_cv', 'gluon_sd']\n",
    "synthetic_df = pd.DataFrame(synthetic_data.cpu().numpy(), columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x  gluon_cv  gluon_sd\n",
      "0  0.498872  0.101677  0.354555\n",
      "1  0.522283  0.091723  0.390013\n",
      "2  0.452715  0.056384 -0.148304\n",
      "3  0.550166  0.146122 -0.042480\n",
      "4  0.727594  0.597430 -0.381009\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first few rows of your synthetic data\n",
    "print(synthetic_df.head())\n",
    "\n",
    "# save the synthetic data to a CSV file\n",
    "synthetic_df.to_csv('synthetic_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
