{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transfor training data from standard source\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # normalize image to [-1, 1]\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "batch_size = 8\n",
    "# dataloader for batch training (mini-batch gradient descent)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "#train_path = './data'\n",
    "#trainloader = DataLoader(\n",
    "                         #torchvision.datasets.ImageFolder (train_path, transform = transform),\n",
    "                         #batch_size = 10, shuffle = True)\n",
    "# 10 classes in total\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data (note that the data has been transformed already)\n",
    "test_images = torch.load('./test_image.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get a random batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(images.shape, labels)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  set the hyperparameters of CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "# the number of output channels is simply equal to the number of filters used in that layer.\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5) # input channel=3, out_put channels / num of filter=6, kernel/ size of filter=5*5\n",
    "        self.pool = nn.MaxPool2d(2, 2) # reduce the image size as factor 2, with a stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # input channel is 6, out put 16, and the filter size is 5*5\n",
    "#Output features = [(Input features + 2 * padding - kernel_size) / stride] + 1\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 16*5*5 is input feautures and 120 is the outputfeaures\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743054a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00211, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_epoch = 3\n",
    "for epoch in range(total_epoch):  # loop over the dataset 'total_epoch' times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0): # for each batch of data\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs) # forward pass\n",
    "        loss = criterion(outputs, labels) # calc loss\n",
    "        loss.backward() # back propagation\n",
    "        optimizer.step() # one step gradient descent\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print average loss every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since now we're testing (not training), we set no_grad to NOT calculate the gradients\n",
    "with torch.no_grad():\n",
    "    # calculate outputs by running images through the network\n",
    "    outputs = model(test_images)\n",
    "    # the class with the highest probability is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = np.array([classes[i] for i in predicted])\n",
    "    \n",
    "print(predicted)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(test_images[:4]))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['label'] = predicted\n",
    "submission.to_csv(\"submission.csv\", index=True, index_label='id')\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e2a97",
   "metadata": {},
   "source": [
    "# Model type: CNN\n",
    "meaning of the hyperparameters as follows:\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5) # input channel=3, out_put channels / num of filter=6, kernel/ size of filter=5*5\n",
    "        self.pool = nn.MaxPool2d(2, 2) # reduce the image size as factor 2, with a stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # input channel is 6, out put 16, and the filter size is 5*5\n",
    "       #Output features = [(Input features + 2 * padding - kernel_size) / stride] + 1\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 16*5*5 is input feautures and 120 is the outputfeaures\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd027545",
   "metadata": {},
   "source": [
    "# Number of Iterations (Epochs):\n",
    "\n",
    "There are 3 iterations/ epochs. It determines how many times the entire training dataset is processed by the model during training.\n",
    "One epoch consists of one forward pass (computing predictions), one backward pass (computing gradients), and one weight update. In each epoch, the model learns from the entire dataset.\n",
    "I increase the No of epochs from 1 to 3 in this model and the accuracy score incresed from 0.49 to 0.57. I noticed that when I increased the epoch no, the loss for each epoch decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d96ea",
   "metadata": {},
   "source": [
    "Learning rate:\n",
    "I started with increasing the learning rate from 0.0005 to 0.01 because it was too small and would have resulted in a slow convergence rate. The learning rate of 0.01 was too high and caused the model to overshoot the optimal solution. I then adjusted the learning rate to 0.001 and gradually increased it to 0.002. This balance between overshooting and convergence speed worked well in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e12a49",
   "metadata": {},
   "source": [
    "Optimzie method:\n",
    "I use Stochastic Gradient Descent (SGD) as the optimize method because it has a faster convergence.\n",
    "This generally helps me updating the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e6fa7",
   "metadata": {},
   "source": [
    "My Findings:\n",
    "A Pipeline for Image Classification**\n",
    "\n",
    "In my image classification task, I've employed a well-established pipeline to achieve accurate results. Here are the key steps I've followed:\n",
    "\n",
    "1. Data Preprocessing:\n",
    "   - First, I load and transform the image data from raw vectors into tensors. This transformation is crucial for compatibility with deep learning models.\n",
    "   - To achieve this, I utilize data loaders and a composition of transformers. These transformers include operations like horizontal flipping, normalization, and conversion to tensors.\n",
    "   - For instance, horizontal flipping helps diversify the training data, while normalization ensures the input values are within a consistent range.\n",
    "   \n",
    "   \n",
    "  transform = transforms.Compose([\n",
    "       transforms.RandomHorizontalFlip(),\n",
    "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "       transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "\n",
    "2. Model Selection and Hyperparameters:\n",
    "   - Next, I select a Convolutional Neural Network (CNN) model that suits my specific task. I've noticed that increasing the number of output channels in the model architecture often leads to improved performance.( Need to learn this parameters thing further as well, the formua behind this??)\n",
    "   - Setting hyperparameters like learning rates, batch sizes, and model depth plays a crucial role in model training. Experimentation helps identify the best values.\n",
    "\n",
    "   model = CNN(num_output_channels=64, ...)\n",
    "   optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    " \n",
    "\n",
    "3. Loss Function and Optimization:\n",
    "   - I define a loss function. Cross-entropy loss is my choiceof use in image classification tasks.\n",
    "   - For optimization, I employ the Stochastic Gradient Descent (SGD) method. SGD is known for its speedy convergence and adaptability to large datasets.\n",
    "  \n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "4. Training:\n",
    "   - With the setup complete, I specify the number of epochs for training. Increasing the number of epochs typically leads to improved accuracy on the training data, although it's essential to monitor for overfitting.\n",
    "   \n",
    "   num_epochs = 3\n",
    "   for epoch in range(num_epochs):\n",
    "       train_model(...)\n",
    "\n",
    "5. Testing:\n",
    "   - Finally, I employ the trained model to make predictions on the test data, evaluating its performance on unseen samples.\n",
    "\n",
    "6. result.the model reaches the accuracy of prediciting of 0.57.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
